{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPV, FPV, AUC, confusion matrix, NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "- Classification Accuracy\n",
    "- AUC â€“ ROC\n",
    "- Confusion Matrix\n",
    "- NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset in use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are demonstrated for classification machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pima Indians Diabetes Data Set: https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem where all of the input variables are numeric.\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description: **\n",
    "\n",
    "The diagnostic, binary-valued variable investigated is whether the\n",
    "patient shows signs of diabetes according to World Health Organization\n",
    "criteria. Pima Indians are a group of Native Americans living near Phoenix, Arizona, USA.\n",
    "\n",
    "Their ADAP algorithm, an adaptive learning routine that generates and executes digital analogs of perceptron-like devices, makes a real-valued prediction between\n",
    "0 and 1.  This was transformed into a binary decision using a cutoff of \n",
    "0.448.  Using 576 training instances, the sensitivity and specificity\n",
    "of their algorithm was 76% on the remaining 192 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6.   ,  148.   ,   72.   , ...,    0.627,   50.   ,    1.   ],\n",
       "       [   1.   ,   85.   ,   66.   , ...,    0.351,   31.   ,    0.   ],\n",
       "       [   8.   ,  183.   ,   64.   , ...,    0.672,   32.   ,    1.   ],\n",
       "       ..., \n",
       "       [   5.   ,  121.   ,   72.   , ...,    0.245,   30.   ,    0.   ],\n",
       "       [   1.   ,  126.   ,   60.   , ...,    0.349,   47.   ,    1.   ],\n",
       "       [   1.   ,   93.   ,   70.   , ...,    0.315,   23.   ,    0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = dataframe.values\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the attributes by data values and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8] # [preg, plas, pres, skin, test, mass, pedi, age]\n",
    "Y = array[:,8] # [class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics evaluate the same algorithms, Logistic Regression for classification. A 10-fold cross-validation test harness is used to demonstrate each metric, because this is the most likely scenario where you will be employing different algorithm evaluation metrics.\n",
    "\n",
    "Cross_val_score function is used to report the performance in each metric. All scores are reported so that they can be sorted in ascending order (largest score is best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification Accuracy\n",
    "= number of correct predictions made as a ratio of all predictions made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.951% (4.841%)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification Accuracy\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" %(results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Area Under ROC Curve (AUC)\n",
    "= performance metric for binary classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"irc_mi\" src=\"https://kaggle2.blob.core.windows.net/competitions/inclass/4050/media/AUC.jpeg\" onload=\"google.aft&amp;&amp;google.aft(this)\" width=\"385\" height=\"281\" style=\"margin-top: 5px;\" alt=\"Image result for Area under roc curve\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC represents a modelâ€™s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.823 (0.041)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification ROC AUC\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows AUC is relatively close to 1 and greater than 0.5, suggesting some skill in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix\n",
    "= presentation of the accuracy of a model with two or more classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"irc_mi\" src=\"https://de.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/60900/versions/11/screenshot.png\" onload=\"google.aft&amp;&amp;google.aft(this)\" width=\"385\" height=\"254\" style=\"margin-top: 1px;\" alt=\"Image result for confusion matrix\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a table that presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm.\n",
    "\n",
    "For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction=0 and actual=0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual=1. And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate a confusion matrix for a set of prediction by a model on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[141  21]\n",
      " [ 41  51]]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalized Discounted Cumulative Gain (NDCG)\n",
    "= measures the performance of a recommendation system based on the graded relevance of the recommended entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal ranking of the entities. This metric is commonly used in information retrieval and to evaluate the performance of web search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"irc_mi\" src=\"http://imageclef.org/system/files/ndcg.png\" onload=\"google.aft&amp;&amp;google.aft(this)\" width=\"385\" height=\"94\" style=\"margin-top: 1px;\" alt=\"Image result for ndcg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download https://github.com/lefterav/rankeval.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should modify directory to where you downloaded it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "rank_eval_dir = r'C:\\Users\\Joon Park\\Downloads\\rankeval-master\\rankeval-master\\src'\n",
    "sys.path.append(rank_eval_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest part of Data Science (maybe programming in general) is schlepping data. \n",
    "\n",
    "Basically, you pass these metrics an ordered list (that's the Rank part). Actually, you need two. The first is a prediction (the output of your model), the second is a reference (probably some simple transformation of the input). The metrics give you back some measure of how good your predicted ranking is.\n",
    "\n",
    "RankEval comes with a class and some utility functions to help define the Ranking. You'll need to wrap your input in an instance of this to use the metrics. Here's an example of how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentence.ranking import Ranking\n",
    "\n",
    "reference_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "predicted_list = [2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11]\n",
    "\n",
    "reference_ranking = Ranking(reference_list)\n",
    "predicted_ranking = Ranking(predicted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two versions of each metric included in the library: an aggregate (set) version and an individual version. \n",
    "\n",
    "ndgc_err() calculates the normalize Discounted Cumulative Gain and the Expected Reciprocal Rank on a sentence level. This follows the definition of DCG and ERR, and the implementation of Yahoo Learning to Rank challenge. It builds on the previous code block, where you create your Ranking objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg:  0.8736656376016968\n"
     ]
    }
   ],
   "source": [
    "from evaluation.ranking.segment import ndgc_err\n",
    "\n",
    "result = ndgc_err(predicted_ranking, reference_ranking)\n",
    "\n",
    "ndcg = result[0]\n",
    "err = result[1]\n",
    "\n",
    "print(\"ndcg: \", ndcg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from evaluation.ranking.set import kendall_tau_set\n",
    "\n",
    "# make a second example prediction\n",
    "predicted_list_two = [12, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1]\n",
    "predicted_ranking_two = Ranking(predicted_list_two)\n",
    "\n",
    "predicted_set = [predicted_ranking, predicted_ranking_two]\n",
    "reference_set = [reference_ranking, reference_ranking]\n",
    "\n",
    "result_set = kendall_tau_set(predicted_set, reference_set)\n",
    "\n",
    "result_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
