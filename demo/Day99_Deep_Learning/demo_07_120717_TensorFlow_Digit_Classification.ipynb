{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.**\n",
    "\n",
    "**Your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data Description:\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n",
    "\n",
    "\n",
    "        000 001 002 003 ... 026 027\n",
    "        028 029 030 031 ... 054 055\n",
    "        056 057 058 059 ... 082 083\n",
    "         |   |   |   |  ...  |   |\n",
    "        728 729 730 731 ... 754 755\n",
    "        756 757 758 759 ... 782 783 \n",
    "        \n",
    "        \n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "Your submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\n",
    "\n",
    "\n",
    "        ImageId,Label\n",
    "        1,3\n",
    "        2,7\n",
    "        3,8 \n",
    "        (27997 more lines)\n",
    "        \n",
    "The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv. file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv file we are using can be found here: https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in training data from train.csv\n",
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTrainFeatureVectors = dfTrain.drop(['label'], axis=1)\n",
    "trainFeatureVectors = dfTrainFeatureVectors.values.astype(dtype=np.float32)\n",
    "trainFeatureVectorsConvoFormat = trainFeatureVectors.reshape(42000, 28, 28, 1)\n",
    "\n",
    "trainLabelsList = dfTrain['label'].tolist()\n",
    "ohTrainLabelsTensor = tf.one_hot(trainLabelsList, depth=10)\n",
    "ohTrainLabelsNdarray = tf.Session().run(ohTrainLabelsTensor).astype(dtype=np.float64)\n",
    "\n",
    "# Read in testing data from test.csv\n",
    "dfTest = pd.read_csv('test.csv')\n",
    "testFeatureVectors = dfTest.values.astype(dtype=np.float32)\n",
    "testFeatureVectorsConvoFormat = testFeatureVectors.reshape(28000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display image from csv. file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us verify that we have read in and structured the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHwiK8ANBEX4gqL8DzPGN7RPqI5UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e92091400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display an image read in from the CSV\n",
    "# testFeatureVectorsConvoFormat values are: [2, 0, 9, 0, 3, 7, ...]\n",
    "pixels = testFeatureVectorsConvoFormat[0].reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Tensorflow graph\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "K = 6  # first convolutional layer output depth\n",
    "L = 12  # second convolutional layer output depth\n",
    "M = 24  # third convolutional layer\n",
    "N = 200  # fully connected layer (softmax)\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "\n",
    "# The model\n",
    "stride = 1  # output is 28x28\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2  # output is 14x14\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2  # output is 7x7\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "Ylogits = tf.matmul(YY4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define calculations we need from the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "predictions = tf.argmax(Y, 1)\n",
    "\n",
    "# training step, the learning rate is a placeholder\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network <font color = red>(Warning: this process roughly takes 30 min!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: training accuracy:0.08 training loss: 4085.32 (lr:0.003)\n",
      "0: ********* test accuracy:0.1148 test loss: 4359.82\n",
      "100: training accuracy:0.9 training loss: 36.4862 (lr:0.0028585653310520707)\n",
      "200: training accuracy:0.93 training loss: 24.9983 (lr:0.0027240285123042826)\n",
      "300: training accuracy:0.94 training loss: 21.2822 (lr:0.0025960531316326675)\n",
      "400: training accuracy:0.91 training loss: 20.272 (lr:0.0024743191839261473)\n",
      "500: training accuracy:0.98 training loss: 4.96255 (lr:0.002358522270907074)\n",
      "500: ********* test accuracy:0.9693 test loss: 9.46531\n",
      "600: training accuracy:0.99 training loss: 7.10621 (lr:0.002248372839976982)\n",
      "700: training accuracy:0.95 training loss: 14.2587 (lr:0.002143595460184269)\n",
      "800: training accuracy:1.0 training loss: 3.26408 (lr:0.002043928133503354)\n",
      "900: training accuracy:0.99 training loss: 4.02043 (lr:0.001949121639703143)\n",
      "1000: training accuracy:1.0 training loss: 1.97057 (lr:0.0018589389131666372)\n",
      "1000: ********* test accuracy:0.9829 test loss: 5.33722\n",
      "1100: training accuracy:0.98 training loss: 8.55107 (lr:0.0017731544501034114)\n",
      "1200: training accuracy:0.99 training loss: 7.96713 (lr:0.001691553744672677)\n",
      "1300: training accuracy:0.96 training loss: 5.66979 (lr:0.0016139327526069466)\n",
      "1400: training accuracy:0.98 training loss: 6.68456 (lr:0.0015400973809950877)\n",
      "1500: training accuracy:0.99 training loss: 1.97035 (lr:0.0014698630029489428)\n",
      "1500: ********* test accuracy:0.9879 test loss: 3.74625\n",
      "1600: training accuracy:0.99 training loss: 6.55623 (lr:0.0014030539959399427)\n",
      "1700: training accuracy:1.0 training loss: 0.831073 (lr:0.0013395033026513076)\n",
      "1800: training accuracy:1.0 training loss: 1.39629 (lr:0.0012790520132477375)\n",
      "1900: training accuracy:0.99 training loss: 4.03135 (lr:0.0012215489680180538)\n",
      "2000: training accuracy:0.99 training loss: 2.52833 (lr:0.0011668503793971828)\n",
      "2000: ********* test accuracy:0.9922 test loss: 2.10148\n",
      "2100: training accuracy:0.99 training loss: 2.40553 (lr:0.0011148194724223506)\n",
      "2200: training accuracy:0.99 training loss: 2.97501 (lr:0.0010653261427244307)\n",
      "2300: training accuracy:0.98 training loss: 2.59139 (lr:0.0010182466311992545)\n",
      "2400: training accuracy:1.0 training loss: 0.262567 (lr:0.0009734632145453863)\n",
      "2500: training accuracy:1.0 training loss: 0.869006 (lr:0.0009308639108945514)\n",
      "2500: ********* test accuracy:0.997 test loss: 0.874177\n",
      "2600: training accuracy:0.99 training loss: 1.40225 (lr:0.0008903421997986366)\n",
      "2700: training accuracy:1.0 training loss: 0.355387 (lr:0.0008517967558730855)\n",
      "2800: training accuracy:1.0 training loss: 0.873842 (lr:0.0008151311954306589)\n",
      "2900: training accuracy:1.0 training loss: 0.130577 (lr:0.0007802538354720133)\n",
      "3000: training accuracy:1.0 training loss: 0.0493774 (lr:0.0007470774644304465)\n",
      "3000: ********* test accuracy:0.9974 test loss: 0.790002\n",
      "3100: training accuracy:1.0 training loss: 0.39724 (lr:0.0007155191240975549)\n",
      "3200: training accuracy:1.0 training loss: 0.162048 (lr:0.0006854999021845007)\n",
      "3300: training accuracy:0.99 training loss: 1.39537 (lr:0.000656944735000187)\n",
      "3400: training accuracy:1.0 training loss: 0.409561 (lr:0.0006297822197529307)\n",
      "3500: training accuracy:1.0 training loss: 0.429389 (lr:0.000603944436006291)\n",
      "3500: ********* test accuracy:0.9982 test loss: 0.565783\n",
      "3600: training accuracy:1.0 training loss: 0.057512 (lr:0.000579366775842601)\n",
      "3700: training accuracy:1.0 training loss: 0.111442 (lr:0.0005559877823095201)\n",
      "3800: training accuracy:1.0 training loss: 0.147819 (lr:0.0005337489957456418)\n",
      "3900: training accuracy:1.0 training loss: 1.14744 (lr:0.0005125948076008895)\n",
      "4000: training accuracy:1.0 training loss: 0.817578 (lr:0.0004924723213861769)\n",
      "4000: ********* test accuracy:0.998 test loss: 0.569557\n",
      "4100: training accuracy:1.0 training loss: 0.164268 (lr:0.0004733312204046323)\n",
      "4200: training accuracy:1.0 training loss: 0.108247 (lr:0.00045512364193364755)\n",
      "4300: training accuracy:1.0 training loss: 0.120226 (lr:0.00043780405754314123)\n",
      "4400: training accuracy:1.0 training loss: 0.376566 (lr:0.00042132915925076824)\n",
      "4500: training accuracy:1.0 training loss: 0.0304265 (lr:0.00040565775122940656)\n",
      "4500: ********* test accuracy:0.9985 test loss: 0.393456\n",
      "4600: training accuracy:1.0 training loss: 0.0408767 (lr:0.0003907506467961309)\n",
      "4700: training accuracy:1.0 training loss: 0.209703 (lr:0.0003765705704250939)\n",
      "4800: training accuracy:1.0 training loss: 0.00950647 (lr:0.0003630820645392963)\n",
      "4900: training accuracy:1.0 training loss: 0.134713 (lr:0.00035025140084817447)\n",
      "5000: training accuracy:1.0 training loss: 0.0109652 (lr:0.00033804649600930654)\n",
      "5000: ********* test accuracy:0.9999 test loss: 0.10209\n",
      "5100: training accuracy:1.0 training loss: 0.00272185 (lr:0.0003264368314033442)\n",
      "5200: training accuracy:1.0 training loss: 0.0102713 (lr:0.0003153933768215683)\n",
      "5300: training accuracy:1.0 training loss: 0.0703633 (lr:0.00030488851787524585)\n",
      "5400: training accuracy:1.0 training loss: 0.335377 (lr:0.0002948959869452743)\n",
      "5500: training accuracy:1.0 training loss: 0.0362799 (lr:0.00028539079749945195)\n",
      "5500: ********* test accuracy:0.9999 test loss: 0.0621415\n",
      "5600: training accuracy:1.0 training loss: 0.0229552 (lr:0.00027634918161313215)\n",
      "5700: training accuracy:1.0 training loss: 0.0123504 (lr:0.0002677485305370315)\n",
      "5800: training accuracy:1.0 training loss: 0.368824 (lr:0.000259567338163581)\n",
      "5900: training accuracy:1.0 training loss: 0.00399933 (lr:0.00025178514725045394)\n",
      "6000: training accuracy:1.0 training loss: 0.0239024 (lr:0.00024438249826680544)\n",
      "6000: ********* test accuracy:0.9997 test loss: 0.112829\n",
      "6100: training accuracy:1.0 training loss: 0.202808 (lr:0.00023734088073430873)\n",
      "6200: training accuracy:1.0 training loss: 0.0965258 (lr:0.00023064268694131766)\n",
      "6300: training accuracy:1.0 training loss: 0.0240538 (lr:0.00022427116791441654)\n",
      "6400: training accuracy:1.0 training loss: 0.0291237 (lr:0.00021821039153726202)\n",
      "6500: training accuracy:1.0 training loss: 0.088236 (lr:0.00021244520271199385)\n",
      "6500: ********* test accuracy:1.0 test loss: 0.0486676\n",
      "6600: training accuracy:1.0 training loss: 0.0195394 (lr:0.00020696118546359606)\n",
      "6700: training accuracy:1.0 training loss: 0.0613714 (lr:0.0002017446268924506)\n",
      "6800: training accuracy:1.0 training loss: 0.0223226 (lr:0.00019678248288494563)\n",
      "6900: training accuracy:1.0 training loss: 0.000578961 (lr:0.00019206234549639704)\n",
      "7000: training accuracy:1.0 training loss: 0.0224753 (lr:0.00018757241192472366)\n",
      "7000: ********* test accuracy:1.0 test loss: 0.0437031\n",
      "7100: training accuracy:1.0 training loss: 0.00185591 (lr:0.00018330145499729437)\n",
      "7200: training accuracy:1.0 training loss: 0.00342 (lr:0.00017923879509714843)\n",
      "7300: training accuracy:1.0 training loss: 0.00494354 (lr:0.0001753742734583905)\n",
      "7400: training accuracy:1.0 training loss: 0.0389163 (lr:0.00017169822676398424)\n",
      "7500: training accuracy:1.0 training loss: 0.298615 (lr:0.00016820146298242642)\n",
      "7500: ********* test accuracy:1.0 test loss: 0.0244649\n",
      "7600: training accuracy:1.0 training loss: 0.0159071 (lr:0.00016487523838288026)\n",
      "7700: training accuracy:1.0 training loss: 0.0176221 (lr:0.0001617112356712938)\n",
      "7800: training accuracy:1.0 training loss: 0.00510882 (lr:0.00015870154319283275)\n",
      "7900: training accuracy:1.0 training loss: 0.0114574 (lr:0.00015583863514862209)\n",
      "8000: training accuracy:1.0 training loss: 0.00861665 (lr:0.00015311535277732913)\n",
      "8000: ********* test accuracy:1.0 test loss: 0.01465\n",
      "8100: training accuracy:1.0 training loss: 0.0337356 (lr:0.0001505248864545312)\n",
      "8200: training accuracy:1.0 training loss: 0.0672266 (lr:0.00014806075866510764)\n",
      "8300: training accuracy:1.0 training loss: 0.0598595 (lr:0.00014571680780607802)\n",
      "8400: training accuracy:1.0 training loss: 0.00757622 (lr:0.00014348717277938534)\n",
      "8500: training accuracy:1.0 training loss: 0.00173384 (lr:0.00014136627833609785)\n",
      "8500: ********* test accuracy:1.0 test loss: 0.0484171\n",
      "8600: training accuracy:1.0 training loss: 0.0153917 (lr:0.00013934882113538272)\n",
      "8700: training accuracy:1.0 training loss: 0.0765783 (lr:0.00013742975648339164)\n",
      "8800: training accuracy:1.0 training loss: 0.00537248 (lr:0.00013560428571889846)\n",
      "8900: training accuracy:1.0 training loss: 0.0268838 (lr:0.0001338678442141468)\n",
      "9000: training accuracy:1.0 training loss: 0.000138399 (lr:0.0001322160899609027)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000: ********* test accuracy:1.0 test loss: 0.0192981\n",
      "9100: training accuracy:1.0 training loss: 0.0233165 (lr:0.00013064489271317272)\n",
      "9200: training accuracy:1.0 training loss: 0.000895155 (lr:0.0001291503236594374)\n",
      "9300: training accuracy:1.0 training loss: 0.00165068 (lr:0.00012772864559857617)\n",
      "9400: training accuracy:1.0 training loss: 0.00243403 (lr:0.00012637630359491788)\n",
      "9500: training accuracy:1.0 training loss: 0.00496684 (lr:0.00012508991608904985)\n",
      "9500: ********* test accuracy:1.0 test loss: 0.0136868\n",
      "9600: training accuracy:1.0 training loss: 0.00491069 (lr:0.0001238662664421581)\n",
      "9700: training accuracy:1.0 training loss: 0.0258318 (lr:0.00012270229489275475)\n",
      "9800: training accuracy:1.0 training loss: 0.00359211 (lr:0.00012159509090568058)\n",
      "9900: training accuracy:1.0 training loss: 0.00180536 (lr:0.00012054188589425116)\n",
      "10000: training accuracy:1.0 training loss: 0.00529116 (lr:0.00011954004629734786)\n",
      "10000: ********* test accuracy:1.0 test loss: 0.0101416\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "def getBatch(i, size, trainFeatures, trainLabels):\n",
    "    startIndex = (i * size) % 42000\n",
    "    endIndex = startIndex + size\n",
    "    batch_X = trainFeatures[startIndex : endIndex]\n",
    "    batch_Y = trainLabels[startIndex : endIndex]\n",
    "    return batch_X, batch_Y\n",
    "\n",
    "# You can call this function in a loop to train the model, 100 images at a time\n",
    "def training_step(i):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    size = 100\n",
    "    batch_X, batch_Y = getBatch(i, size, trainFeatureVectorsConvoFormat, ohTrainLabelsNdarray)\n",
    "\n",
    "    # learning rate decay\n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "\n",
    "    # compute training values\n",
    "    if i % 100 == 0:\n",
    "        '''\n",
    "        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on batch_X and batch_Y (ie. on 100 pieces of data)\n",
    "        '''\n",
    "        a, c = sess.run([accuracy, cross_entropy], {X: batch_X, Y_: batch_Y, pkeep: 1.0})\n",
    "        print(str(i) + \": training accuracy:\" + str(a) + \" training loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "\n",
    "    # compute test values\n",
    "    if i % 500 == 0:\n",
    "        '''\n",
    "        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on all of the data\n",
    "        '''\n",
    "        a, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorsConvoFormat[-10000:], Y_: ohTrainLabelsNdarray[-10000:], pkeep: 1.0})\n",
    "        print(str(i) + \": ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, pkeep: 0.75})\n",
    "\n",
    "# Run number of iterations training the NN    \n",
    "for i in range(10000+1): \n",
    "    training_step(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********* test accuracy:1.0 test loss: 0.010089\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>27971</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27971</th>\n",
       "      <td>27972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27972</th>\n",
       "      <td>27973</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>27974</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27974</th>\n",
       "      <td>27975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27975</th>\n",
       "      <td>27976</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27976</th>\n",
       "      <td>27977</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>27978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27978</th>\n",
       "      <td>27979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27979</th>\n",
       "      <td>27980</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27980</th>\n",
       "      <td>27981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>27982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>27983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>27984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>27985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27985</th>\n",
       "      <td>27986</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27986</th>\n",
       "      <td>27987</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27987</th>\n",
       "      <td>27988</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27988</th>\n",
       "      <td>27989</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27989</th>\n",
       "      <td>27990</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27990</th>\n",
       "      <td>27991</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>27992</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27992</th>\n",
       "      <td>27993</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>27994</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>27995</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "5            6      7\n",
       "6            7      0\n",
       "7            8      3\n",
       "8            9      0\n",
       "9           10      3\n",
       "10          11      5\n",
       "11          12      7\n",
       "12          13      4\n",
       "13          14      0\n",
       "14          15      4\n",
       "15          16      3\n",
       "16          17      3\n",
       "17          18      1\n",
       "18          19      9\n",
       "19          20      0\n",
       "20          21      9\n",
       "21          22      1\n",
       "22          23      1\n",
       "23          24      5\n",
       "24          25      7\n",
       "25          26      4\n",
       "26          27      2\n",
       "27          28      7\n",
       "28          29      4\n",
       "29          30      7\n",
       "...        ...    ...\n",
       "27970    27971      5\n",
       "27971    27972      0\n",
       "27972    27973      4\n",
       "27973    27974      8\n",
       "27974    27975      0\n",
       "27975    27976      3\n",
       "27976    27977      6\n",
       "27977    27978      0\n",
       "27978    27979      1\n",
       "27979    27980      9\n",
       "27980    27981      3\n",
       "27981    27982      1\n",
       "27982    27983      1\n",
       "27983    27984      0\n",
       "27984    27985      4\n",
       "27985    27986      5\n",
       "27986    27987      2\n",
       "27987    27988      2\n",
       "27988    27989      9\n",
       "27989    27990      6\n",
       "27990    27991      7\n",
       "27991    27992      6\n",
       "27992    27993      5\n",
       "27993    27994      9\n",
       "27994    27995      7\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the test accurscy on some data that was held out\n",
    "a, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorsConvoFormat[-10000:], Y_: ohTrainLabelsNdarray[-10000:], pkeep: 1.0})\n",
    "print(\"\\n ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "# Get predictions on test data\n",
    "p = sess.run([predictions], {X: testFeatureVectorsConvoFormat, pkeep: 1.0})\n",
    "\n",
    "# Write predictions to csv file\n",
    "results = pd.DataFrame({'ImageId': pd.Series(range(1, len(p[0]) + 1)), 'Label': pd.Series(p[0])})\n",
    "#results.to_csv('results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
